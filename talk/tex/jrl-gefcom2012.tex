\input{../include/header_beamer}

\usecolortheme{default}
\xdefinecolor{Black}{rgb}{0,0,0}
\xdefinecolor{White}{rgb}{1,1,1}
\xdefinecolor{DarkBlue}{rgb}{0,0,.7}
\xdefinecolor{DarkRed}{rgb}{.7,0,0}
\xdefinecolor{Red}{rgb}{.85,0,0}
\xdefinecolor{DarkGreen}{rgb}{0,.7,0}
\xdefinecolor{DarkMagenta}{rgb}{.6,0,.6}
\def\Black{\textcolor{Black}}
\def\White{\textcolor{White}}
\def\Blue{\textcolor{DarkBlue}}
\def\Magenta{\textcolor{DarkMagenta}}
\def\Red{\textcolor{Red}}
\def\Green{\textcolor{DarkGreen}}
\definecolor{camlightblue}{rgb}{0.601 , 0.8, 1}

\input{../include/commenting.tex}

%% For submission, make all render blank.
%\renewcommand{\LATER}[1]{}
%\renewcommand{\fLATER}[1]{}
%\renewcommand{\TBD}[1]{}
%\renewcommand{\fTBD}[1]{}
%\renewcommand{\PROBLEM}[1]{}
%\renewcommand{\fPROBLEM}[1]{}
%\renewcommand{\NA}[1]{#1}  %% Note, NA's pass through!

\usepackage{alltt}
\usepackage{psfrag}
\usepackage{pstool}
\usepackage{multicol}
\usepackage{tabularx}
\usepackage{preamble}

\def\newarrow{\mbox{\begin{tikzpicture}
             \useasboundingbox{(-3pt,-4.5pt) rectangle (19pt,1pt)};
             \draw[->] (0,-0.07)--(17pt,-0.07);\end{tikzpicture}}}
             
\setbeamertemplate{background} {\includegraphics[width=\paperwidth,height=\paperheight,keepaspectratio]{2010-PES-PPT-Template-v2007.pdf}}

\title[] % (optional, use only with long paper titles)
{GEFCom2012 Hierarchical load forecasting:\\Gradient boosting machines and Gaussian processes}

\author % (optional, use only with lots of authors)
{James Robert Lloyd}
% - Use the \inst{?} command only if the authors have different
%   affiliation.

\institute[] % (optional, but mostly needed)
{Machine Learning Group,\\Department of Engineering,\\University of Cambridge}
% - Use the \inst command only if there are several affiliations.
% - Keep it simple, no one is interested in your street address.

\date % (optional)
{July 2013}

\subject{Talks}

\usetikzlibrary{shapes.geometric,arrows,chains,matrix,positioning,scopes}
 \makeatletter
 \tikzset{join/.code=\tikzset{after node path={%
       \ifx\tikzchainprevious\pgfutil@empty\else(\tikzchainprevious)%
       edge[every join]#1(\tikzchaincurrent)\fi}}
 }
 \tikzset{>=stealth',every on chain/.append style={join},
   every join/.style={->}
 }

\tikzstyle{mybox} = [draw=none, rectangle]
\usepackage{ifthen}
\usepackage{booktabs}

% Custom definitions
\def\simiid{\sim_{\mbox{\tiny iid}}}
\def\ie{i.e.\ }
\def\eg{e.g.\ }

%%%%
% Paper specific stuff
%%%%

\begin{document}

\small
%% { 
%%   \setbeamertemplate{footline}{\empty}
%%   \begin{frame}
%%     \titlepage
%%   \end{frame}
%% }
%\renewcommand{\inserttotalframenumber}{11}

%\input{../include/defs}

\begin{frame}
  %\begin{block}{}
    \titlepage
  %\end{block}
  \begin{center}
    {\bf  Thanks to}\\
    Alex Davies\\
    David Duvenaud\\
    Zoubin Ghahramani
  \end{center}
\end{frame}

\begin{frame}{Overview of techniques}
  \begin{block}{Preprocessing}
    %\vspace{\baselineskip}
    \begin{itemize}
      \item Kernel smoothing of temperatures
    \end{itemize}
  \end{block}
  \vspace{\baselineskip}
  \begin{block}{Temperature forecasting}
    %\vspace{\baselineskip}
    \begin{itemize}
      \item Gaussian process (GP) regression
    \end{itemize}
  \end{block}
  \vspace{\baselineskip}
  \begin{block}{Load back/forecasting}
    %\vspace{\baselineskip}
    \begin{itemize}
      \item Gradient boosting machine (GBM) regression  - 76\%
      \item Gaussian process (GP) regression - 14\%
      \item Linear regression (benchmark solution) - 10\%
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}{Performance of Different Components}
  \begin{center}
    \begin{tabular}{lcc|ccc}
      Method &&&&& Validation score\\&&&&&\\
      \hline&&&&&\\
      GBM &&&&& 72,968 \\&&&&&\\
      GP &&&&& 99,787 \\&&&&&\\
      LR &&&&& 112,547 \\&&&&&\\
      \hline&&&&&\\
      Ensemble &&&&& 71,164
    \end{tabular}
  \end{center}
\end{frame}

%\begin{frame}{Notation}
%  \begin{itemize}
%    \item $t$ --- Time
%    \vspace{\baselineskip}
%    \item $T$ --- Temperature
%    \vspace{\baselineskip}
%    \item $S$ --- Kernel smoothed temperature
%    \vspace{\baselineskip}
%    \item $Z$ --- Load
%    \vspace{\baselineskip}
%    \item $f,g$ --- Generic functions
%    \vspace{\baselineskip}
%    \item $\varepsilon$ --- Generic error
%  \end{itemize}
%\end{frame}

\begin{frame}{Main approach - regression on time and temp.}
  \begin{block}{Modelling temperatures and loads as functions of time and temperature}
    \vspace{-1\baselineskip}
    \begin{eqnarray*}
      T(t) & = & f(t, \bar{S}(t)) + \varepsilon^T_t \\
      \\
      Z(t) & = & g(t, T(t), S(t)) + \varepsilon^Z_t \\
    \end{eqnarray*}
    \vspace{-2\baselineskip}
  \end{block}
  \begin{block}{\ie no explicit autoregressive components \eg}
    \vspace{-1\baselineskip}
    \begin{eqnarray*}
      y(t+1) & = & f(y(t)) + \varepsilon_t
    \end{eqnarray*}
  \end{block}
  \begin{block}{Notation}
    $t$ --- Time, $T$ --- Temperature, $S$ --- Smoothed temperature $\bar{S}$ --- Historical average of smoothed temperature, $Z$ --- Load, $f,g$ --- Generic functions, $\varepsilon$ --- Generic error
  \end{block}
\end{frame}

\begin{frame}{GBM regression - overview}
  \begin{block}{Used as a regression `black-box'}
    \begin{itemize}
      \item Bagged and boosted decision trees
      \item Used standard R implementation with most default parameters unchanged
    \end{itemize}
  \end{block}
  \begin{block}{Output}
  	\begin{itemize}
      \item $Z_i$ \ie each load zone modelled in isolation
    \end{itemize}
  \end{block}
  \begin{block}{Inputs}
    \begin{itemize}
      \item Time of day
      \item Time within week
      \item Temperatures (all stations)
      \item Smoothed temperatures (all stations)
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}{GBM regression - parameter selection}
  \begin{itemize}
    \item Ideally would have performed grid searches over parameter values using cross validated error as metric
    \vspace{\baselineskip}
    \item In practice, partial grid searches combined with intuition, using out of bag errors and validation score on Kaggle
    \vspace{\baselineskip}
    \item 10,000 trees, interaction depth of 3 and shrinkage factor of 0.01 (other values set to defaults of R implementation)
  \end{itemize}
\end{frame}

\begin{frame}{GBM regression - example}
  \vspace{-2\baselineskip}
  \begin{center}
    \input{../include/gbm}
  \end{center}
  \vspace{-1\baselineskip}
  Only slight discontinuity between prediction and ground truth despite no explicit modelling assumptions of continuity
\end{frame}

\begin{frame}{Gaussian process regression}
  \begin{itemize}
    \item A Bayesian nonparametric method for regression
    \vspace{\baselineskip}
    \item Places a prior on functions but equivalent to linear regression in an (infinite dimensional) feature space
    \vspace{\baselineskip}
    \item Typically used as a smoothing device by choosing a default \emph{kernel}
    \vspace{\baselineskip}
    \item Data exhibiting high level structure (\eg periodicity) can be modelled using more advanced kernels
  \end{itemize}
\end{frame}

\begin{frame}{Bayesian modelling}
  \begin{block}{Bayes' rule}
    \begin{equation*}
      \mathbb{P}(\textrm{hypothesis}|\textrm{data}) = \frac{\mathbb{P}(\textrm{data}|\textrm{hypothesis})\mathbb{P}(\textrm{hypothesis})}{\mathbb{P}(\textrm{data})}
    \end{equation*}
  \end{block}
  \begin{block}{}
    \begin{itemize}
      \item Bayes' rule follows from basic axioms of probability theory
      \vspace{\baselineskip}
      \item Provides a calculus to update beliefs in response to data
      \vspace{\baselineskip}
      \item Requires the specification of prior beliefs about data - choice of prior is crucial for successful modelling
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}{Prior on functions}
  \begin{center}
    \input{../include/SE}
  \end{center}
\end{frame}

\begin{frame}{Conditional Posterior}
  %\begin{align*}
  %  f(x^*) | X, y \sim \mathcal{N}( & k( x^*, X ) K^{-1} y, \\
  %  & k( x^*, x^* ) - k( x^*, X ) K^{-1} k( X, x^* ) )
  %\end{align*}

  %With SE kernel:
  After observing data, Bayes rule provides a formula by which to update our beliefs about a function
    \begin{figure}
        \includegraphics<1>[width=6cm]{../include/gp_demo/1d_posterior_and_0_data}
        \includegraphics<2>[width=6cm]{../include/gp_demo/1d_posterior_and_1_data}
        \includegraphics<3>[width=6cm]{../include/gp_demo/1d_posterior_and_2_data}
        \includegraphics<4>[width=6cm]{../include/gp_demo/1d_posterior_and_3_data}
        \includegraphics<5>[width=6cm]{../include/gp_demo/1d_posterior_and_4_data}
    \end{figure}
\end{frame}

%\begin{frame}{Need more than just a smoothing device}
%  \begin{center}
%    \input{../include/fig_synth_extrap_bad.tex}
%  \end{center}
%\end{frame}

%\begin{frame}{Need more than just a smoothing device}
%  \begin{center}
%    \input{../include/load_pred_SE}
%  \end{center}
%\end{frame}

\begin{frame}{Encoding structural assumptions}
  \begin{itemize}
    \item Gaussian processes typically used as smoothing devices
    \vspace{\baselineskip}
    \item Daily and weekly periodicity assumptions could be encoded by feature engineering as with GBM
    \vspace{\baselineskip}
    \item However, structural assumptions can also be encoded in the \emph{kernel} of a GP
    \begin{itemize}
      \item Using a different method allows predictions to be uncorrelated - very useful for ensembling
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Can encode structural assumptions in kernel}
  \begin{itemize} 
	\item Kernel determines the structural properties of a Gaussian process
	\item Many different kinds, with very different properties:
  \end{itemize}
  \input{../include/tables/simple_kernels_table_v3}
\end{frame}

\begin{frame}{Kernels can be composed}
  \begin{itemize} 
	\item Two main operations: adding, multiplying
  \end{itemize}
  \input{../include/tables/comp1}
\end{frame}

%\begin{frame}{Need more than just a smoothing device}
%  \begin{center}
%    \input{../include/load_pred_SEard}
%  \end{center}
%\end{frame}

\begin{frame}{A suitable prior}
  %\begin{itemize}
    %\item Used kernels of the form $\SE_t + \SE_S + \SE_t \times \SE_T \times \Per_t$
    %\item
  Used structured kernel to encode the assumption that
  \begin{eqnarray*}
    \textrm{Load} & = & \textrm{Smooth function of time} +  \\
    & & \textrm{Smooth function of smoothed temperatures} + \\
    & & \textrm{Daily periodicity smoothly changing with time and temperature}
  \end{eqnarray*}
  %\end{itemize}
  \begin{center}
    \input{../include/structured_prior}
  \end{center}
\end{frame}

\begin{frame}{GP regression - example}
    \begin{eqnarray*}
    \textrm{Load} & = & \textrm{Smooth function of time} +  \\
    & & \textrm{Smooth function of smoothed temperatures} + \\
    & & \textrm{Daily periodicity smoothly changing with time and temperature}
  \end{eqnarray*}
  \begin{center}
    \input{../include/load_pred}
  \end{center}
\end{frame}

%\begin{frame}{GP regression - without periodicity}
%    \begin{eqnarray*}
%    \textrm{Load} & = & \textrm{Smooth function of time} +  \\
%    & & \textrm{Smooth function of temperatures} + \\
%    & & \textrm{Smooth function of smoothed temperatures}
%  \end{eqnarray*}
%  \begin{center}
%    \input{../include/load_pred_SEard}
%  \end{center}
%\end{frame}
%
%\begin{frame}{GP regression - without structure}
%  \vspace{-1\baselineskip}
%    \begin{eqnarray*}
%    \textrm{Load} & = & \textrm{Smooth function of time}
%  \end{eqnarray*}
%  \begin{center}
%    \input{../include/load_pred_SE_small}
%  \end{center}
%  Suitable prior assumptions crucial when using any Bayesian method
%\end{frame}

\begin{frame}{Structured kernels for temp.\ forecasts}
  %\begin{itemize}
  %  \item Assumed that Temperature = Smoothed historical average + Smooth long-term deviations + Daily periodicity
  %\end{itemize}
  \begin{eqnarray*}
    \textrm{Temperature} & = & \textrm{Smooth historical average temperature} +  \\
    & & \textrm{Smooth long-term deviations} + \\
    & & \textrm{Daily periodicity}
  \end{eqnarray*}
  \begin{center}
    \input{../include/temp_pred}
  \end{center}
\end{frame}

\begin{frame}{GP regression - parameter selection}
  \begin{itemize}
    \item Can optimise marginal likelihood (a balance of model fit and complexity) with gradient based optimisation
    \vspace{\baselineskip}
    \item Marginal likelihood optimisation can fail
    \begin{itemize}
      \item Can result in slight over fitting
      \item When the prior and data generation process are dissimilar, Bayesian inference can give misleading results
    \end{itemize}
    \vspace{\baselineskip}
    \item In practice, parameter selection was a mixture of marginal likelihood optimisation, validation score maximisation and model checking (plotting graphs)
  \end{itemize}
\end{frame}

\begin{frame}{Competition inspired new GP research}
  \begin{itemize}
    \item Creating custom composite kernels not a new idea, but typically only practised by GP / kernel learning experts
    \vspace{\baselineskip}
    \item After competition, automated the process of kernel / model construction \cite{duvenaud2013structure} based on an idea by \cite{Grosse2012} in the context of matrix factorisation
    \vspace{\baselineskip}
    \item Ongoing research to see how far the automatic model construction idea can be pushed \eg
  \end{itemize}
  \begin{tabular}{ccc}
\hspace{-0.5cm}\includegraphics[width=0.33\textwidth]{figures/internet-traffic-data-in-bits-fr/internet-traffic-data-in-bits-fr_all} &
\includegraphics[width=0.32\textwidth]{figures/weekday-bus-ridership-iowa-city-/weekday-bus-ridership-iowa-city-_all} & 
\includegraphics[width=0.32\textwidth]{figures/monthly-production-of-gas-in-aus/monthly-production-of-gas-in-aus_all} \\
\hspace{-0.5cm}\includegraphics[width=0.33\textwidth]{figures/monthly-us-female-20-years-and-o/monthly-us-female-20-years-and-o_all} &
\includegraphics[width=0.32\textwidth]{figures/monthly-canadian-total-unemploym/monthly-canadian-total-unemploym_all} & 
\includegraphics[width=0.32\textwidth]{figures/monthly-sales-of-us-houses-thous/monthly-sales-of-us-houses-thous_all}
  \end{tabular}
\end{frame}

\begin{frame}{Ensembling}
  \begin{block}{Small search over possible weightings}
    \begin{center}
      \scriptsize
      \begin{tabular}{cccc|c}
        GBM & GP & RF & LM & Score \\
        \hline
        100 & 0 & 0 & 0 & 72,968 \\
        0 & 100 & 0 & 0 & 99,787 \\
        0 & 0 & 100 & 0 & 89,457 \\
        0 & 0 & 0 & 100 & 112,547 \\
        80 & 20 & 0 & 0 & 71,683 \\
        70 & 30 & 0 & 0 & 72,485 \\
        90 & 10 & 0 & 0 & 71,846 \\
        85 & 15 & 0 & 0 & 71,644 \\
        \bf{76} & \bf{14} & \bf{0} & \bf{10} & \bf{71,164} \\
        72 & 13 & 10 & 5 & 71,566 \\
        80 & 0 & 20 & 10 & 74,293 \\
        \ldots & \ldots & \ldots & \ldots & \ldots
      \end{tabular}
      \vspace{-1\baselineskip}
    \end{center}
      \vspace{-1\baselineskip}
  \end{block}
  \begin{block}{More principled methods}
    \begin{itemize}
      \item Grid searches and cross validation - but could be costly to retrain algorithms on different training / test splits
      \item Bayesian optimisation \cite{Osborne2009}, \cite{snoek2012practical}, \cite{HennigSchuler2012} can be more appropriate when individual evaluations costly (\eg submitting to Kaggle to obtain validation score)
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}{Summary}
  \begin{itemize}
    \item Main approach was to regress loads on time and temperature, rather than using an autoregressive model
    \vspace{2\baselineskip}
    \item GBM provided the majority of performance
    \vspace{2\baselineskip}
    \item Structured kernel GP method sufficiently uncorrelated to provide useful component in ensemble
  \end{itemize}
\end{frame}

{
\section{References}
\section{Extended Bibliography}
\tiny
\begin{frame}[allowframebreaks,plain]
  \frametitle{References}
  \bibliography{library}
  \bibliographystyle{alpha}
\end{frame}
}

\end{document}